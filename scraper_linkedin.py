import requests
import time
from bs4 import BeautifulSoup
from config import KEYWORDS, LOCATION, MAX_PAGES, REQUEST_DELAY, ZENROWS_API_KEY

ZENROWS_BASE = "https://api.zenrows.com/v1/"

#é€šç”¨è¯·æ±‚å‡½æ•°ï¼Œå¸¦è‡ªåŠ¨é‡è¯•
def zenrows_get(url, retries=3, delay=2):
    for attempt in range(retries):
        try:
            params = {'url': url, 'apikey': ZENROWS_API_KEY}
            r = requests.get(ZENROWS_BASE, params=params, timeout=30)
            if r.status_code == 200:
                return r.text
            else:
                print(f"ZenRows è¯·æ±‚å¤±è´¥ [{r.status_code}] ç¬¬ {attempt + 1} æ¬¡: {url}")
        except Exception as e:
            print(f"è¯·æ±‚å¼‚å¸¸ ç¬¬ {attempt + 1} æ¬¡: {str(e)}")
        time.sleep(delay * (attempt + 1))
    return None


#æŠ“å– LinkedIn æœç´¢ç»“æœ
def fetch_linkedin_list(keyword):
    results = []
    for page in range(MAX_PAGES):
        start = page * 25
        search_url = (
            f"https://www.linkedin.com/jobs/search?"
            f"keywords={keyword.replace(' ', '%20')}&location={LOCATION.replace(' ', '%20')}&start={start}"
        )
        print(f"ğŸ“„ æŠ“å–åˆ—è¡¨é¡µï¼š{keyword} - ç¬¬ {page + 1} é¡µ")
        html = zenrows_get(search_url)
        if not html:
            continue
        soup = BeautifulSoup(html, 'html.parser')
        cards = soup.find_all('div', class_='base-card')
        for card in cards:
            title_tag = card.find('h3', class_='base-search-card__title')
            company_tag = card.find('h4', class_='base-search-card__subtitle')
            location_tag = card.find('span', class_='job-search-card__location')
            date_tag = card.find('time')
            link_tag = card.find('a', class_='base-card__full-link')

            job = {
                'èŒä½åç§°': title_tag.get_text(strip=True) if title_tag else '',
                'å…¬å¸åç§°': company_tag.get_text(strip=True) if company_tag else '',
                'ä¸“ä¸šè¦æ±‚': '',
                'åœ°ç‚¹': location_tag.get_text(strip=True) if location_tag else '',
                'è–ªèµ„è¦æ±‚': '',
                'å·¥ä½œæè¿°': '',
                'å›¢é˜Ÿè§„æ¨¡/ä¸šåŠ¡çº¿è§„æ¨¡': '',
                'å…¬å¸è§„æ¨¡': '',
                'èŒä½å‘å¸ƒæ—¶é—´': date_tag['datetime'] if date_tag and date_tag.has_attr('datetime') else '',
                'èŒä½çŠ¶æ€': 'Active',
                'æ‹›è˜å¹³å°': 'LinkedIn',
                'é“¾æ¥': link_tag['href'] if link_tag and link_tag.has_attr('href') else ''
            }
            results.append(job)
        time.sleep(REQUEST_DELAY)
    return results


#æŠ“å–è¯¦æƒ…é¡µè¡¥å…¨
def enrich_job_details(job_list):
    for idx, job in enumerate(job_list):
        url = job.get('é“¾æ¥')
        if not url:
            continue
        html = zenrows_get(url)
        if not html:
            continue
        soup = BeautifulSoup(html, 'html.parser')

        desc_tag = soup.find('div', class_='show-more-less-html__markup')
        description = desc_tag.get_text(separator=' ', strip=True) if desc_tag else ''
        job['å·¥ä½œæè¿°'] = description

        lower_desc = description.lower()
        if "requirement" in lower_desc or "qualification" in lower_desc:
            req_index = lower_desc.find("requirement")
            job['ä¸“ä¸šè¦æ±‚'] = description[req_index:req_index + 400]

        salary_tag = soup.find('span', string=lambda s: s and '$' in s)
        if salary_tag:
            job['è–ªèµ„è¦æ±‚'] = salary_tag.get_text(strip=True)

        if (idx + 1) % 10 == 0 or idx == len(job_list) - 1:
            print(f"è¯¦æƒ…é¡µè¿›åº¦ï¼š{idx + 1}/{len(job_list)}")
        time.sleep(REQUEST_DELAY)


def fetch_linkedin_jobs(keyword):
    jobs = fetch_linkedin_list(keyword)
    return jobs


def fetch_details_for_all_jobs(unique_jobs):
    print(f"å¼€å§‹æŠ“å–è¯¦æƒ…é¡µï¼Œå…± {len(unique_jobs)} æ¡èŒä½")
    enrich_job_details(unique_jobs)
    print("è¯¦æƒ…é¡µæŠ“å–å®Œæˆ")
